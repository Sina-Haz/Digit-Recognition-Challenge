{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b992319-80d8-443f-9d69-986635bf226c",
   "metadata": {},
   "source": [
    "## Using Micrograd for Digit Recognition\n",
    "\n",
    "Today we will try to train a neural network with micrograd that can recognize digits from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8769a036-e2f4-4422-8e1e-c787fba69eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "#importing our custom library\n",
    "from Micrograd.engine import Value\n",
    "from Micrograd.nn import Neuron, Layer, MLP, Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cff435d-c4a0-4c7a-a28b-b5f1605236e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import the data using pandas -> then we can turn it into a numpy array\n",
    "dig_data = pd.read_csv(\"MNIST_Train.csv\")\n",
    "dig_data = np.array(dig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1886070-694f-4ad0-abf9-81559c5cc769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2eabe-a33c-4122-b85b-bf1ae445b06e",
   "metadata": {},
   "source": [
    "As we can see the imported data is a massive matrix of 42000x785. We need to break this data down.\n",
    "\n",
    "Each row represents a single image. The first column contains all the labels as to what number that image represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f43146-988b-4d39-bb06-70a48f605c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make our results more reproducible by setting random seed\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba20b37a-14eb-4a8e-b43b-bbbf64857e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next we want to shuffle our data and then split it up. Want to shuffle around all the rows\n",
    "np.random.shuffle(dig_data) #shuffles in-place\n",
    "\n",
    "#testing set will be of size 1000\n",
    "Y_test = dig_data[:1000,0]\n",
    "X_test = dig_data[:1000,1:]\n",
    "X_test = X_test / 255\n",
    "\n",
    "Y_train = dig_data[1000:,0]\n",
    "X_train = dig_data[1000:,1:]\n",
    "X_train = X_train / 255\n",
    "\n",
    "#lets store the number of training examples into a variable called m\n",
    "m_train = X_train.shape[0]\n",
    "m_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db920062-220e-4cd4-8950-163860593845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's try to draw up one of the rows of X_train with matplotlib and make sure that the image matches up with the label\n",
    "index = np.random.randint(0,m_train) \n",
    "\n",
    "img = (X_train[index, :, None] * 255).reshape(28,28)\n",
    "\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.show\n",
    "\n",
    "print(Y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a746cf-5d2c-4808-99bc-bfe0544aadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 8070\n"
     ]
    }
   ],
   "source": [
    "#Now let's initialize our neural network using micrograd\n",
    "nn = MLP(784, [10,10,10])\n",
    "print(\"number of parameters\",len(nn.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74cb99-d221-40d0-9fa9-11e0c39872d5",
   "metadata": {},
   "source": [
    "Remember that in order to feed data forward in our neural network we need our inputs to be in the form of Value objects.\n",
    "\n",
    "In order to convert each row of our numpy matrix to a row of Values we can use np.vectorize(), this takes in a method (in our case its Value.__init__) and turns it into a vectorized function that can be applied to each element within a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6e9495-1f95-45a5-a46c-97aefe6c2b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-2.4601407450898263, grad=0),\n",
       " Value(data=-18.78384031756561, grad=0),\n",
       " Value(data=13.524954842973628, grad=0),\n",
       " Value(data=4.4653047796051135, grad=0),\n",
       " Value(data=-9.514971397461334, grad=0),\n",
       " Value(data=1.1703881332967392, grad=0),\n",
       " Value(data=-12.213768538862098, grad=0),\n",
       " Value(data=-5.530389103809711, grad=0),\n",
       " Value(data=-7.911619954299733, grad=0),\n",
       " Value(data=-0.49106457090295924, grad=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating vectorized version of Value() that will work on entire numpy array (this is sort of like map(Value, python_list)\n",
    "To_Val = np.vectorize(Value)\n",
    "\n",
    "#we can call this function on the first row of X\n",
    "nn_1st_inp = To_Val(X_train[0, :])\n",
    "\n",
    "nn(nn_1st_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e59aa3-ac3b-4fae-bc6a-049af6ee9adf",
   "metadata": {},
   "source": [
    "Let's get the models base accuracy on our testing data based off of these randomized weights.\n",
    "\n",
    "To do this we will first need to be able to convert any Y[i] to one-hot encoding (i.e a 10x1 vector of all zeros, except at index Y[i] where it's 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3495680-093a-4a6d-8a5f-1f027befc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot_m(y): #y will be a number from 0 to 9\n",
    "    return [Value(1) if i == y else Value(0) for i in range(10)]\n",
    "\n",
    "def from_one_hot_m(one_hot_y): #returns a number from a one_hot encoded vector\n",
    "    return max(range(len(one_hot_y)), key = lambda i:one_hot_y[i].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1d91c8-1b99-4fcd-8696-97ff55e8bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[Value(data=0, grad=0), Value(data=0, grad=0), Value(data=1, grad=0), Value(data=0, grad=0), Value(data=0, grad=0), Value(data=0, grad=0), Value(data=0, grad=0), Value(data=0, grad=0), Value(data=0, grad=0), Value(data=0, grad=0)]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "index = np.random.randint(0,m_train)\n",
    "\n",
    "label = Y_train[index]\n",
    "ohy = to_one_hot_m(label)\n",
    "reobtained = from_one_hot_m(ohy)\n",
    "\n",
    "print(label)\n",
    "print(ohy)\n",
    "print(reobtained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a87888a-da8b-4ca5-888a-4841108ec316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.079"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAcc(model, X_data = X_test, Y_data = Y_test):\n",
    "    total_ex = Y_data.shape[0]\n",
    "    total_correct = 0\n",
    "\n",
    "    for i in range(total_ex):\n",
    "        row = X_data[i,:]\n",
    "        nn_out = model(row)\n",
    "        if from_one_hot_m(nn_out) == Y_data[i]:\n",
    "            total_correct += 1\n",
    "    return total_correct / total_ex\n",
    "\n",
    "getAcc(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc7a41-c62b-41f8-9503-928707dd18eb",
   "metadata": {},
   "source": [
    "Initially we have a very low accuracy of only 8% \n",
    "\n",
    "Lets see what we can do to change this. First we will need a way to calculate loss. The training data is too large to find the loss over the entire thing. Instead we will have to use stochastic gradient descent with mini-batches to approximate the gradient of the loss function\n",
    "\n",
    "However first I must point out a critical mistake I've made while constructing my model. For my activation I've been using sigmoid to simply squish all the outputs to be between 0 and 1 so I can treat them like probabilities. The problem is, that these are not probability, they don't all add up to 1 so our model is inaccurate at the logical level. In order to get the best results we should have the model give a linear output, and then apply something like the softmax function to it.\n",
    "\n",
    "The Softmax function takes a vector of elements and returns a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "50a0379b-d411-4378-91db-cfaef57970a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes vector of numbers, returns vector of probabilities\n",
    "import math\n",
    "\n",
    "def softmax(vector):\n",
    "    denom = sum(math.exp(i.data) for i in vector) #this is numerical\n",
    "    numerators = [math.exp(i.data) for i in vector] #these are numerical\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += out.data*(1-out.data)*out.grad #derivative of softmax(xi) with respect to xi is softmax(xi)*(1-softmax(xi))\n",
    "\n",
    "    probs = []\n",
    "    for i in range(len(vector)):\n",
    "        self = vector[i]\n",
    "        out = Value(numerators[i]/denom, (vector[i], ), _op = 'softmax')\n",
    "        out._backward = _backward\n",
    "        probs.append(out)\n",
    "    return probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c7eab2d3-ec4a-43ae-bcae-ce26767e3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Micrograd.draw import draw_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d48a082-54cf-45a3-95af-95f5cca5590a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.002529615601878868, grad=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets make sure our softmax is working correctly\n",
    "\n",
    "a, b, c = Value(3), Value(2), Value(-1)\n",
    "\n",
    "vector = [a,b,c]\n",
    "\n",
    "probs = [val.softmax(vector) for val in vector]\n",
    "\n",
    "y = probs[0]*probs[1]*probs[2]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1bb4143-5ea3-493d-aca2-6411148cad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1097pt\" height=\"155pt\"\n",
       " viewBox=\"0.00 0.00 1097.10 155.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 151)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-151 1093.1,-151 1093.1,4 -4,4\"/>\n",
       "<!-- 140338460065856 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140338460065856</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-110.5 0,-146.5 161,-146.5 161,-110.5 0,-110.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.88\" y=\"-123.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"79.75,-111 79.75,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.38\" y=\"-123.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0019</text>\n",
       "</g>\n",
       "<!-- 140338460066768Softmax -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140338460066768Softmax</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"239.65\" cy=\"-128.5\" rx=\"42.65\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.65\" y=\"-123.45\" font-family=\"Times,serif\" font-size=\"14.00\">Softmax</text>\n",
       "</g>\n",
       "<!-- 140338460065856&#45;&gt;140338460066768Softmax -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140338460065856&#45;&gt;140338460066768Softmax</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161,-128.5C169.38,-128.5 177.75,-128.5 185.72,-128.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.69,-132 195.69,-128.5 185.69,-125 185.69,-132\"/>\n",
       "</g>\n",
       "<!-- 140338460069456 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140338460069456</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"641.1,-0.5 641.1,-36.5 802.1,-36.5 802.1,-0.5 641.1,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"680.98\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.0132</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"720.85,-1 720.85,-36.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"761.48\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.1915</text>\n",
       "</g>\n",
       "<!-- 140338460035824* -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140338460035824*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"865.1\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"865.1\" y=\"-40.45\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 140338460069456&#45;&gt;140338460035824* -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140338460069456&#45;&gt;140338460035824*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M802.04,-33.66C811.03,-35.37 819.83,-37.05 827.85,-38.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"827.13,-42.2 837.6,-40.63 828.44,-35.32 827.13,-42.2\"/>\n",
       "</g>\n",
       "<!-- 140338460069456Softmax -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140338460069456Softmax</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"562.45\" cy=\"-18.5\" rx=\"42.65\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"562.45\" y=\"-13.45\" font-family=\"Times,serif\" font-size=\"14.00\">Softmax</text>\n",
       "</g>\n",
       "<!-- 140338460069456Softmax&#45;&gt;140338460069456 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140338460069456Softmax&#45;&gt;140338460069456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M605.57,-18.5C613.36,-18.5 621.78,-18.5 630.38,-18.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"630.35,-22 640.35,-18.5 630.35,-15 630.35,-22\"/>\n",
       "</g>\n",
       "<!-- 140338460066432 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140338460066432</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"641.1,-55.5 641.1,-91.5 802.1,-91.5 802.1,-55.5 641.1,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"680.98\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.1915</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"720.85,-56 720.85,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"761.48\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0132</text>\n",
       "</g>\n",
       "<!-- 140338460066432&#45;&gt;140338460035824* -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140338460066432&#45;&gt;140338460035824*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M802.04,-57.78C811.22,-55.96 820.2,-54.19 828.36,-52.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"828.77,-55.86 837.9,-50.49 827.42,-48.99 828.77,-55.86\"/>\n",
       "</g>\n",
       "<!-- 140338460066432* -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140338460066432*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"562.45\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"562.45\" y=\"-68.45\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 140338460066432*&#45;&gt;140338460066432 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140338460066432*&#45;&gt;140338460066432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M589.76,-73.5C601.29,-73.5 615.57,-73.5 630.41,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"630.29,-77 640.29,-73.5 630.29,-70 630.29,-77\"/>\n",
       "</g>\n",
       "<!-- 140338460065952 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140338460065952</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"318.3,-0.5 318.3,-36.5 483.8,-36.5 483.8,-0.5 318.3,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"360.43\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;1.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"402.55,-1 402.55,-36.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"443.18\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0025</text>\n",
       "</g>\n",
       "<!-- 140338460065952&#45;&gt;140338460069456Softmax -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140338460065952&#45;&gt;140338460069456Softmax</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M483.62,-18.5C492.15,-18.5 500.65,-18.5 508.74,-18.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"508.46,-22 518.46,-18.5 508.46,-15 508.46,-22\"/>\n",
       "</g>\n",
       "<!-- 140338460067056 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140338460067056</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320.55,-55.5 320.55,-91.5 481.55,-91.5 481.55,-55.5 320.55,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"360.43\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.7214</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"400.3,-56 400.3,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"440.93\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0035</text>\n",
       "</g>\n",
       "<!-- 140338460067056&#45;&gt;140338460066432* -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140338460067056&#45;&gt;140338460066432*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M481.28,-73.5C496.39,-73.5 511.52,-73.5 524.5,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"524.3,-77 534.3,-73.5 524.3,-70 524.3,-77\"/>\n",
       "</g>\n",
       "<!-- 140338460067056Softmax -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140338460067056Softmax</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"239.65\" cy=\"-73.5\" rx=\"42.65\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.65\" y=\"-68.45\" font-family=\"Times,serif\" font-size=\"14.00\">Softmax</text>\n",
       "</g>\n",
       "<!-- 140338460067056Softmax&#45;&gt;140338460067056 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140338460067056Softmax&#45;&gt;140338460067056</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.53,-73.5C291.11,-73.5 300.45,-73.5 309.99,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.82,-77 319.82,-73.5 309.82,-70 309.82,-77\"/>\n",
       "</g>\n",
       "<!-- 140338460035824 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140338460035824</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"928.1,-27.5 928.1,-63.5 1089.1,-63.5 1089.1,-27.5 928.1,-27.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"967.98\" y=\"-40.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.0025</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1007.85,-28 1007.85,-63.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1048.48\" y=\"-40.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n",
       "</g>\n",
       "<!-- 140338460035824*&#45;&gt;140338460035824 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140338460035824*&#45;&gt;140338460035824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M892.39,-45.5C899.85,-45.5 908.43,-45.5 917.47,-45.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"917.25,-49 927.25,-45.5 917.25,-42 917.25,-49\"/>\n",
       "</g>\n",
       "<!-- 140338460069648 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140338460069648</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-55.5 0,-91.5 161,-91.5 161,-55.5 0,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.88\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 3.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"79.75,-56 79.75,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.38\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0007</text>\n",
       "</g>\n",
       "<!-- 140338460069648&#45;&gt;140338460067056Softmax -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140338460069648&#45;&gt;140338460067056Softmax</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161,-73.5C169.38,-73.5 177.75,-73.5 185.72,-73.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.69,-77 195.69,-73.5 185.69,-70 185.69,-77\"/>\n",
       "</g>\n",
       "<!-- 140338460066768 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140338460066768</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320.55,-110.5 320.55,-146.5 481.55,-146.5 481.55,-110.5 320.55,-110.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"360.43\" y=\"-123.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.2654</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"400.3,-111 400.3,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"440.93\" y=\"-123.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0095</text>\n",
       "</g>\n",
       "<!-- 140338460066768&#45;&gt;140338460066432* -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140338460066768&#45;&gt;140338460066432*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M457.09,-110.55C466.03,-107.57 475.18,-104.48 483.8,-101.5 498.48,-96.43 514.65,-90.63 528.41,-85.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"529.21,-88.71 537.41,-82 526.81,-82.13 529.21,-88.71\"/>\n",
       "</g>\n",
       "<!-- 140338460066768Softmax&#45;&gt;140338460066768 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140338460066768Softmax&#45;&gt;140338460066768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.53,-128.5C291.11,-128.5 300.45,-128.5 309.99,-128.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.82,-132 319.82,-128.5 309.82,-125 309.82,-132\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fa3180f8880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd1d5bd-70c4-42ef-91ce-6ddb64218198",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bacdf0b-cd5c-4f72-baeb-3f422e8f61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets compare this to pytorch softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f601a7be-43a1-4561-b302-5b30dd9fd11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7214, 0.2654, 0.0132], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp=torch.tensor([3,2,-1]).double()\n",
    "inp.requires_grad = True\n",
    "\n",
    "probs = torch.nn.functional.softmax(inp,dim=0)\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "febf899f-efaf-43da-afff-a7bc2fa2ac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0025, dtype=torch.float64, grad_fn=<ProdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#looks like our forward pass is good for softmax\n",
    "y = torch.prod(probs)\n",
    "print(y)\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f2cea72-dcb3-41e2-a2e7-04ee51f6c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0029,  0.0005,  0.0024], dtype=torch.float64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edf407e2-6352-4abc-9cda-d84597f188a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3.3454944762769903, grad=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It looks like our softmax function isn't calculating gradients correctly. Lets just use sigmoid for now to just see what we can do with it\n",
    "\n",
    "#We'll need a way to find the loss -> lets just use mean sqaure error\n",
    "def mse_loss(model, batch_size):\n",
    "    #get a bunch of random indices and put it into a list\n",
    "    ri = np.random.permutation(m_train)[:batch_size]\n",
    "\n",
    "    Xb,yb = To_Val(X_train[ri]),Y_train[ri]\n",
    "    predictions,actual = [],[]\n",
    "\n",
    "    #get all the predictions and all the actual values\n",
    "    for i in range(len(Xb)):\n",
    "        predictions.append(model(Xb[i]))\n",
    "        actual.append(to_one_hot(yb[i]))\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    #for each prediction the loss is the sum of all the squared losses\n",
    "    for pred,actual in zip(predictions,actual):\n",
    "        loss = sum((pred[i] - actual[i])**2 for i in range(len(pred)))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return sum(losses)/len(losses) #get average of all the losses\n",
    "    \n",
    "\n",
    "mse_loss(nn,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aeb612b0-9bc4-49cc-a05c-87e05c52d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we can get loss lets make a descent function\n",
    "def descent(model,batch_size,num_steps):\n",
    "    for k in range(num_steps):\n",
    "        loss = mse_loss(model,batch_size)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        learning_rate = 0.1\n",
    "        for p in model.parameters():\n",
    "            p.data -= learning_rate*p.grad\n",
    "\n",
    "        if k%25 == 0:\n",
    "            print(f\"step: {k}, loss: {loss.data}, accuracy: {getAcc(nn) *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d1439a0-86ee-4695-9dff-3f69fcf817c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.621300327259827, accuracy: 41.0%\n",
      "step: 25, loss: 0.733732472359752, accuracy: 41.0%\n",
      "step: 50, loss: 1.064172696272161, accuracy: 39.0%\n",
      "step: 75, loss: 0.41805934266594175, accuracy: 41.0%\n",
      "step: 100, loss: 0.7982320246492957, accuracy: 39.0%\n",
      "step: 125, loss: 0.9251171813062836, accuracy: 38.0%\n",
      "step: 150, loss: 0.5788964270778124, accuracy: 42.0%\n",
      "step: 175, loss: 0.8030421078133169, accuracy: 43.0%\n"
     ]
    }
   ],
   "source": [
    "descent(nn, 5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d3cf06a-7f01-4bb4-86d5-d435a6a13ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.6359232096399109, accuracy: 42.0%\n",
      "step: 25, loss: 0.6383618273824933, accuracy: 42.0%\n",
      "step: 50, loss: 0.6994849655912214, accuracy: 41.0%\n",
      "step: 75, loss: 0.5511540721440962, accuracy: 42.0%\n"
     ]
    }
   ],
   "source": [
    "descent(nn, 35, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1226b440-fa84-4332-ab38-c18d1145af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.6682907901545447, accuracy: 42.0%\n",
      "step: 25, loss: 0.6210965218771894, accuracy: 41.0%\n",
      "step: 50, loss: 0.6568152791123262, accuracy: 43.0%\n",
      "step: 75, loss: 0.6538115842514789, accuracy: 43.0%\n"
     ]
    }
   ],
   "source": [
    "descent(nn, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543d34d-dcdd-49db-affb-323227983961",
   "metadata": {},
   "source": [
    "After a lot of training (close to a 1000 steps) and a lot of time (~3 hours) our model only achieves 43% accuracy. This is likely because of problems such as:\n",
    " - Using sigmoid instead of softmax and therefore not having a proper probability distribution in the output layer\n",
    " - Using a very computationally inefficient scalar engine for a neural network with so many parameters\n",
    " - using very small batches and not doing a lot of steps because of the inefficiency and time it takes for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09a66b-0241-4bbb-b074-6a48a3ed2a80",
   "metadata": {},
   "source": [
    "## Using Vectorgrad for Digit Recognition\n",
    "\n",
    "To resolve all of these problems I've made Vectorgrad which uses the underlying graph structure and simple API of a micrograd or pytorch but incorporates numpy and vector and matrix operations and calculus so that we can minimize the number of parameters in our model and do calculations in parallel to drastically decrease our compute time. The syntax for using is almost the same as before but now we have a Vector object rather than a value object (to be more accurate perhaps I should have called it a Tensor object but as of right some operations don't support tensors of a higher order than 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e8ea69-f9b1-46d4-92cc-ad9b9c19c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now use our vectorized nn functions\n",
    "from Vectorgrad import Vector\n",
    "from Vectorgrad import Module, Layer, MLP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77be5f8c-cc91-48dd-8eda-667d6b51690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [relu Layer(number of Neurons = 10), relu Layer(number of Neurons = 10), softmax Layer(number of Neurons = 10)]\n",
      "Number of parameters:  6\n"
     ]
    }
   ],
   "source": [
    "new_nn = MLP(784, [10, 10, 10], [Vector.relu, Vector.relu, Vector.softmax])\n",
    "print(new_nn)\n",
    "print('Number of parameters: ', len(new_nn.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50621f54-0f3c-426b-8948-355fa1913520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Vector(data=[ 0.  0.  0.  0. nan nan  0.  0.  0.  0.], grad=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]),\n",
       " Vector(data=[0.0000000e+00 4.2963551e-05 2.0547289e-31 7.9594118e-34 2.7294144e-01\n",
       "  7.2701550e-01 3.0497656e-30 0.0000000e+00 0.0000000e+00 4.1898824e-43], grad=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]),\n",
       " Vector(data=[ 0.  0.  0.  0.  0. nan  0.  0.  0.  0.], grad=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[new_nn(X_train[i]) for i in range(3)] #testing feed forward of our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0766c108-a8ae-44ae-a299-730e5e101635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y): #y will be a number from 0 to 9\n",
    "    encoded = np.zeros(10)\n",
    "    encoded[y] = 1\n",
    "    return Vector(encoded)\n",
    "\n",
    "def from_one_hot(pred): #get the y value from a vector that is in one hot format\n",
    "    return np.argmax(pred.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0699a270-295b-42e6-917f-2bb61e7ac2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAccuracy(model, X = X_test, Y = Y_test):\n",
    "    correct, num_ins = 0, X.shape[0]\n",
    "    for i in range(num_ins):\n",
    "        if Y[i] == from_one_hot(model(X[i])):\n",
    "            correct+=1\n",
    "    return correct/num_ins\n",
    "\n",
    "getAccuracy(new_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a304439a-8351-49bb-ae20-ce094bbb203e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(data=1.5410032272338867, grad=0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.random.randint(0, m_train)\n",
    "out = new_nn(X_train[index])\n",
    "correct = to_one_hot(Y_train[index])\n",
    "\n",
    "((out-correct)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdf11c09-4722-4430-8474-c2a9c2b18e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(data=1.780347466468811, grad=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We'll need a way to find the loss -> lets just use mean sqaure error\n",
    "def mse_loss(model, batch_size, X = X_train, Y = Y_train):\n",
    "    #get a bunch of random indices and put it into a list\n",
    "    ri = np.random.permutation(m_train)[:batch_size]\n",
    "\n",
    "    Xb,yb = X[ri],Y[ri]\n",
    "    model_pred = [model(Xb[i]) for i in range(batch_size)] #Matrix or array of vectors\n",
    "    actual_vals = [to_one_hot(yb[i]) for i in range(batch_size)]\n",
    "    losses = []\n",
    "    \n",
    "    #for each prediction the loss is the sum of all the squared losses\n",
    "    for pred,actual in zip(model_pred,actual_vals):\n",
    "        loss = ((pred - actual)**2).sum() #we do element-wise subtraction and then raise to 2nd power then we sum up all the losses in the array to get a scalar\n",
    "        if not np.isnan(loss.data): losses.append(loss)\n",
    "\n",
    "    avg_loss = sum(losses)/batch_size\n",
    "    return avg_loss #get average of all the losses\n",
    "\n",
    "mse_loss(new_nn, 32) #sometimes we get nan, not sure why could be because of overflow/underflow\n",
    "#I changed my function so that we disregard values that get us nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abd43d60-c411-4111-a25d-de056531f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(model,batch_size,num_steps,loss_fn = mse_loss):\n",
    "    for k in range(num_steps):\n",
    "        loss = loss_fn(model,batch_size)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        learning_rate = 0.1\n",
    "        for p in model.parameters():\n",
    "            p.data -= learning_rate*p.grad\n",
    "\n",
    "        if k%500 == 0:\n",
    "            print(f\"step: {k}, loss: {loss.data}, accuracy: {getAccuracy(model) *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb2d861-cc38-44b5-a0e5-d7d36d94d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 1.7279458045959473, accuracy: 6.0%\n",
      "step: 25, loss: 1.8444972038269043, accuracy: 8.0%\n",
      "step: 50, loss: 1.7423914670944214, accuracy: 8.0%\n",
      "step: 75, loss: 1.3616341352462769, accuracy: 10.0%\n",
      "step: 100, loss: 1.6750316619873047, accuracy: 14.000000000000002%\n",
      "step: 125, loss: 1.6799017190933228, accuracy: 17.0%\n",
      "step: 150, loss: 1.5622918605804443, accuracy: 18.0%\n",
      "step: 175, loss: 1.5885342359542847, accuracy: 16.0%\n",
      "step: 200, loss: 1.2761650085449219, accuracy: 17.0%\n",
      "step: 225, loss: 1.5438363552093506, accuracy: 16.0%\n",
      "step: 250, loss: 1.407946228981018, accuracy: 18.0%\n",
      "step: 275, loss: 1.282869577407837, accuracy: 20.0%\n",
      "step: 300, loss: 1.1974008083343506, accuracy: 22.0%\n",
      "step: 325, loss: 1.1958634853363037, accuracy: 22.0%\n",
      "step: 350, loss: 1.1446665525436401, accuracy: 22.0%\n",
      "step: 375, loss: 1.0375326871871948, accuracy: 25.0%\n",
      "step: 400, loss: 0.9061558246612549, accuracy: 25.0%\n",
      "step: 425, loss: 0.9799529910087585, accuracy: 27.0%\n",
      "step: 450, loss: 0.8422524929046631, accuracy: 28.000000000000004%\n",
      "step: 475, loss: 0.9075674414634705, accuracy: 27.0%\n",
      "step: 500, loss: 0.9466158747673035, accuracy: 26.0%\n",
      "step: 525, loss: 0.8991343379020691, accuracy: 27.0%\n",
      "step: 550, loss: 0.8163400292396545, accuracy: 30.0%\n",
      "step: 575, loss: 0.7086891531944275, accuracy: 30.0%\n",
      "step: 600, loss: 0.8055092692375183, accuracy: 28.999999999999996%\n",
      "step: 625, loss: 0.8831433653831482, accuracy: 30.0%\n",
      "step: 650, loss: 0.8067362904548645, accuracy: 31.0%\n",
      "step: 675, loss: 0.8053413033485413, accuracy: 33.0%\n",
      "step: 700, loss: 0.8691752552986145, accuracy: 35.0%\n",
      "step: 725, loss: 0.7929715514183044, accuracy: 31.0%\n",
      "step: 750, loss: 0.7733302712440491, accuracy: 31.0%\n",
      "step: 775, loss: 0.8795209527015686, accuracy: 30.0%\n",
      "step: 800, loss: 0.7280091643333435, accuracy: 32.0%\n",
      "step: 825, loss: 0.7754957675933838, accuracy: 33.0%\n",
      "step: 850, loss: 0.8147252202033997, accuracy: 33.0%\n",
      "step: 875, loss: 0.7980479001998901, accuracy: 32.0%\n",
      "step: 900, loss: 0.753343939781189, accuracy: 32.0%\n",
      "step: 925, loss: 0.7517618536949158, accuracy: 34.0%\n",
      "step: 950, loss: 0.7739325761795044, accuracy: 33.0%\n",
      "step: 975, loss: 0.7555429935455322, accuracy: 33.0%\n",
      "step: 1000, loss: 0.8289412260055542, accuracy: 30.0%\n",
      "step: 1025, loss: 0.8813349604606628, accuracy: 33.0%\n",
      "step: 1050, loss: 0.7757813334465027, accuracy: 36.0%\n",
      "step: 1075, loss: 0.7490177750587463, accuracy: 32.0%\n",
      "step: 1100, loss: 0.8484358787536621, accuracy: 35.0%\n",
      "step: 1125, loss: 0.891656756401062, accuracy: 32.0%\n",
      "step: 1150, loss: 0.8105682730674744, accuracy: 39.0%\n",
      "step: 1175, loss: 0.7127231359481812, accuracy: 38.0%\n",
      "step: 1200, loss: 0.7482355833053589, accuracy: 34.0%\n",
      "step: 1225, loss: 0.7278597354888916, accuracy: 37.0%\n",
      "step: 1250, loss: 0.7803332209587097, accuracy: 39.0%\n",
      "step: 1275, loss: 0.8673546314239502, accuracy: 39.0%\n",
      "step: 1300, loss: 0.8175116777420044, accuracy: 40.0%\n",
      "step: 1325, loss: 0.7137458920478821, accuracy: 36.0%\n",
      "step: 1350, loss: 0.8250792026519775, accuracy: 41.0%\n",
      "step: 1375, loss: 0.8998841047286987, accuracy: 44.0%\n",
      "step: 1400, loss: 0.6529465317726135, accuracy: 40.0%\n",
      "step: 1425, loss: 0.8267197012901306, accuracy: 45.0%\n",
      "step: 1450, loss: 0.8769155740737915, accuracy: 42.0%\n",
      "step: 1475, loss: 0.7573171257972717, accuracy: 42.0%\n",
      "step: 1500, loss: 0.7567090392112732, accuracy: 46.0%\n",
      "step: 1525, loss: 0.7685232162475586, accuracy: 46.0%\n",
      "step: 1550, loss: 0.8268470764160156, accuracy: 45.0%\n",
      "step: 1575, loss: 0.6659104824066162, accuracy: 42.0%\n",
      "step: 1600, loss: 0.716195821762085, accuracy: 41.0%\n",
      "step: 1625, loss: 0.8986470103263855, accuracy: 45.0%\n",
      "step: 1650, loss: 0.7976401448249817, accuracy: 47.0%\n",
      "step: 1675, loss: 0.690263569355011, accuracy: 47.0%\n",
      "step: 1700, loss: 0.7193947434425354, accuracy: 49.0%\n",
      "step: 1725, loss: 0.6858101487159729, accuracy: 47.0%\n",
      "step: 1750, loss: 0.6100592017173767, accuracy: 50.0%\n",
      "step: 1775, loss: 0.7697727084159851, accuracy: 54.0%\n",
      "step: 1800, loss: 0.7067174315452576, accuracy: 50.0%\n",
      "step: 1825, loss: 0.7231563329696655, accuracy: 50.0%\n",
      "step: 1850, loss: 0.66839599609375, accuracy: 51.0%\n",
      "step: 1875, loss: 0.7575210332870483, accuracy: 49.0%\n",
      "step: 1900, loss: 0.6730077862739563, accuracy: 54.0%\n",
      "step: 1925, loss: 0.5823301672935486, accuracy: 52.0%\n",
      "step: 1950, loss: 0.6577110290527344, accuracy: 50.0%\n",
      "step: 1975, loss: 0.7485156059265137, accuracy: 51.0%\n",
      "step: 2000, loss: 0.5496546626091003, accuracy: 56.00000000000001%\n",
      "step: 2025, loss: 0.7716143131256104, accuracy: 54.0%\n",
      "step: 2050, loss: 0.5777691602706909, accuracy: 54.0%\n",
      "step: 2075, loss: 0.7133567929267883, accuracy: 53.0%\n",
      "step: 2100, loss: 0.6327756643295288, accuracy: 53.0%\n",
      "step: 2125, loss: 0.7082398533821106, accuracy: 56.99999999999999%\n",
      "step: 2150, loss: 0.46232327818870544, accuracy: 54.0%\n",
      "step: 2175, loss: 0.6980614066123962, accuracy: 60.0%\n",
      "step: 2200, loss: 0.6951553225517273, accuracy: 56.00000000000001%\n",
      "step: 2225, loss: 0.6286277174949646, accuracy: 55.00000000000001%\n",
      "step: 2250, loss: 0.7522766590118408, accuracy: 56.00000000000001%\n",
      "step: 2275, loss: 0.6089826822280884, accuracy: 56.00000000000001%\n",
      "step: 2300, loss: 0.6186959147453308, accuracy: 49.0%\n",
      "step: 2325, loss: 0.581342875957489, accuracy: 56.00000000000001%\n",
      "step: 2350, loss: 0.6725798845291138, accuracy: 56.99999999999999%\n",
      "step: 2375, loss: 0.6881076097488403, accuracy: 54.0%\n",
      "step: 2400, loss: 0.6193917393684387, accuracy: 54.0%\n",
      "step: 2425, loss: 0.6137110590934753, accuracy: 55.00000000000001%\n",
      "step: 2450, loss: 0.597400426864624, accuracy: 55.00000000000001%\n",
      "step: 2475, loss: 0.737677812576294, accuracy: 52.0%\n",
      "step: 2500, loss: 0.6084959506988525, accuracy: 56.00000000000001%\n",
      "step: 2525, loss: 0.5019592642784119, accuracy: 51.0%\n",
      "step: 2550, loss: 0.589896023273468, accuracy: 56.00000000000001%\n",
      "step: 2575, loss: 0.530655562877655, accuracy: 57.99999999999999%\n",
      "step: 2600, loss: 0.7057130336761475, accuracy: 57.99999999999999%\n",
      "step: 2625, loss: 0.5125752091407776, accuracy: 56.00000000000001%\n",
      "step: 2650, loss: 0.5741038918495178, accuracy: 56.00000000000001%\n",
      "step: 2675, loss: 0.5557819604873657, accuracy: 55.00000000000001%\n",
      "step: 2700, loss: 0.5122419595718384, accuracy: 57.99999999999999%\n",
      "step: 2725, loss: 0.5460774898529053, accuracy: 60.0%\n",
      "step: 2750, loss: 0.6119515299797058, accuracy: 60.0%\n",
      "step: 2775, loss: 0.6222580671310425, accuracy: 56.99999999999999%\n",
      "step: 2800, loss: 0.642526388168335, accuracy: 57.99999999999999%\n",
      "step: 2825, loss: 0.5044330358505249, accuracy: 54.0%\n",
      "step: 2850, loss: 0.5880354046821594, accuracy: 55.00000000000001%\n",
      "step: 2875, loss: 0.48375204205513, accuracy: 57.99999999999999%\n",
      "step: 2900, loss: 0.6414212584495544, accuracy: 56.00000000000001%\n",
      "step: 2925, loss: 0.5483652949333191, accuracy: 60.0%\n",
      "step: 2950, loss: 0.5935920476913452, accuracy: 60.0%\n",
      "step: 2975, loss: 0.46994566917419434, accuracy: 59.0%\n",
      "step: 3000, loss: 0.4646936357021332, accuracy: 56.00000000000001%\n",
      "step: 3025, loss: 0.7335494160652161, accuracy: 57.99999999999999%\n",
      "step: 3050, loss: 0.553054928779602, accuracy: 56.99999999999999%\n",
      "step: 3075, loss: 0.5732950568199158, accuracy: 56.99999999999999%\n",
      "step: 3100, loss: 0.48356109857559204, accuracy: 56.99999999999999%\n",
      "step: 3125, loss: 0.6477072238922119, accuracy: 56.99999999999999%\n",
      "step: 3150, loss: 0.6764508485794067, accuracy: 61.0%\n",
      "step: 3175, loss: 0.5708891749382019, accuracy: 56.99999999999999%\n",
      "step: 3200, loss: 0.47546935081481934, accuracy: 61.0%\n",
      "step: 3225, loss: 0.41677650809288025, accuracy: 59.0%\n",
      "step: 3250, loss: 0.6340873837471008, accuracy: 60.0%\n",
      "step: 3275, loss: 0.5605030059814453, accuracy: 59.0%\n",
      "step: 3300, loss: 0.551715075969696, accuracy: 62.0%\n",
      "step: 3325, loss: 0.5327008366584778, accuracy: 60.0%\n",
      "step: 3350, loss: 0.5127813816070557, accuracy: 63.0%\n",
      "step: 3375, loss: 0.5834907293319702, accuracy: 60.0%\n",
      "step: 3400, loss: 0.5212792754173279, accuracy: 60.0%\n",
      "step: 3425, loss: 0.5282321572303772, accuracy: 57.99999999999999%\n",
      "step: 3450, loss: 0.41293367743492126, accuracy: 60.0%\n",
      "step: 3475, loss: 0.4701095223426819, accuracy: 59.0%\n",
      "step: 3500, loss: 0.5200399160385132, accuracy: 63.0%\n",
      "step: 3525, loss: 0.418117880821228, accuracy: 63.0%\n",
      "step: 3550, loss: 0.4770204424858093, accuracy: 63.0%\n",
      "step: 3575, loss: 0.6409773826599121, accuracy: 59.0%\n",
      "step: 3600, loss: 0.5153000950813293, accuracy: 62.0%\n",
      "step: 3625, loss: 0.4974549412727356, accuracy: 61.0%\n",
      "step: 3650, loss: 0.470731645822525, accuracy: 64.0%\n",
      "step: 3675, loss: 0.5333085060119629, accuracy: 63.0%\n",
      "step: 3700, loss: 0.40507692098617554, accuracy: 64.0%\n",
      "step: 3725, loss: 0.4360554814338684, accuracy: 67.0%\n",
      "step: 3750, loss: 0.5406196713447571, accuracy: 62.0%\n",
      "step: 3775, loss: 0.5237753391265869, accuracy: 64.0%\n",
      "step: 3800, loss: 0.5064012408256531, accuracy: 65.0%\n",
      "step: 3825, loss: 0.4676876366138458, accuracy: 63.0%\n",
      "step: 3850, loss: 0.5272303819656372, accuracy: 64.0%\n",
      "step: 3875, loss: 0.5572007894515991, accuracy: 65.0%\n",
      "step: 3900, loss: 0.3975285291671753, accuracy: 59.0%\n",
      "step: 3925, loss: 0.46850380301475525, accuracy: 65.0%\n",
      "step: 3950, loss: 0.3250381052494049, accuracy: 63.0%\n",
      "step: 3975, loss: 0.4092860221862793, accuracy: 66.0%\n",
      "step: 4000, loss: 0.4926016628742218, accuracy: 65.0%\n",
      "step: 4025, loss: 0.47360652685165405, accuracy: 64.0%\n",
      "step: 4050, loss: 0.4997520446777344, accuracy: 63.0%\n",
      "step: 4075, loss: 0.6079044342041016, accuracy: 68.0%\n",
      "step: 4100, loss: 0.51507568359375, accuracy: 64.0%\n",
      "step: 4125, loss: 0.4837929308414459, accuracy: 65.0%\n",
      "step: 4150, loss: 0.4705134332180023, accuracy: 70.0%\n",
      "step: 4175, loss: 0.5106601119041443, accuracy: 66.0%\n",
      "step: 4200, loss: 0.4333188831806183, accuracy: 61.0%\n",
      "step: 4225, loss: 0.5290177464485168, accuracy: 62.0%\n",
      "step: 4250, loss: 0.5519533753395081, accuracy: 63.0%\n",
      "step: 4275, loss: 0.5454978942871094, accuracy: 66.0%\n",
      "step: 4300, loss: 0.4795405864715576, accuracy: 64.0%\n",
      "step: 4325, loss: 0.503531277179718, accuracy: 61.0%\n",
      "step: 4350, loss: 0.4619048535823822, accuracy: 62.0%\n",
      "step: 4375, loss: 0.5899407267570496, accuracy: 67.0%\n",
      "step: 4400, loss: 0.7078388333320618, accuracy: 65.0%\n",
      "step: 4425, loss: 0.4404488801956177, accuracy: 66.0%\n",
      "step: 4450, loss: 0.47245481610298157, accuracy: 66.0%\n",
      "step: 4475, loss: 0.5194217562675476, accuracy: 64.0%\n",
      "step: 4500, loss: 0.45113617181777954, accuracy: 67.0%\n",
      "step: 4525, loss: 0.4945991039276123, accuracy: 68.0%\n",
      "step: 4550, loss: 0.5813853144645691, accuracy: 69.0%\n",
      "step: 4575, loss: 0.46737101674079895, accuracy: 67.0%\n",
      "step: 4600, loss: 0.4829884171485901, accuracy: 66.0%\n",
      "step: 4625, loss: 0.42253029346466064, accuracy: 65.0%\n",
      "step: 4650, loss: 0.6615762710571289, accuracy: 63.0%\n",
      "step: 4675, loss: 0.5608612895011902, accuracy: 64.0%\n",
      "step: 4700, loss: 0.5066314935684204, accuracy: 64.0%\n",
      "step: 4725, loss: 0.4263665974140167, accuracy: 66.0%\n",
      "step: 4750, loss: 0.47908815741539, accuracy: 66.0%\n",
      "step: 4775, loss: 0.3786744475364685, accuracy: 64.0%\n",
      "step: 4800, loss: 0.4843546450138092, accuracy: 65.0%\n",
      "step: 4825, loss: 0.4559021294116974, accuracy: 65.0%\n",
      "step: 4850, loss: 0.6155434846878052, accuracy: 66.0%\n",
      "step: 4875, loss: 0.5700684189796448, accuracy: 65.0%\n",
      "step: 4900, loss: 0.4480198919773102, accuracy: 65.0%\n",
      "step: 4925, loss: 0.6220776438713074, accuracy: 65.0%\n",
      "step: 4950, loss: 0.5785781741142273, accuracy: 67.0%\n",
      "step: 4975, loss: 0.4675498902797699, accuracy: 67.0%\n"
     ]
    }
   ],
   "source": [
    "descent(new_nn, 50, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9d850-3df5-46b9-901a-2ccf1e9c0dd0",
   "metadata": {},
   "source": [
    "## Our new model is exponentially faster and is approaching better accuracy. Here are some possible reasons that it's not there yet:\n",
    "1) We are using MSE which is not really the best loss function for multiple classification tasks like this, instead we should implement cross entropy loss\n",
    "2) 2000 steps is likely nowhere near enough steps for our model to train on (also depends on batch size too)\n",
    "3) Perhaps we need more neurons in the first or second layer or perhaps we should change up the activation functions used in each layer\n",
    "4) We should also use algorithms to avoid overfitting such as L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce363f4-f9ee-4be0-9cde-84a10013532c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6524343675417661"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see our models accuracy on the training data -> 59% not bad\n",
    "getAccuracy(new_nn, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b5cb4-565c-43e3-806b-bd2a582b1819",
   "metadata": {},
   "source": [
    "Lets now work to implement cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57117a76-8eaa-4886-a4b9-1c0557f6938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, batch_size, X = X_train, Y = Y_train, epsilon = 1e-10):\n",
    "    ri = np.random.permutation(m_train)[:batch_size]\n",
    "\n",
    "    Xb,yb = X[ri],Y[ri]\n",
    "    model_pred = [model(Xb[i]) for i in range(batch_size)] #Matrix of vectors \n",
    "    actual_vals = [to_one_hot(yb[i]) for i in range(batch_size)]\n",
    "    losses = []\n",
    "\n",
    "    for pred, actual in zip(model_pred, actual_vals):\n",
    "        log_probs = (pred + epsilon).log()\n",
    "        log_probs *= actual\n",
    "        cross_entropy = -log_probs.sum()\n",
    "        if not np.isnan(cross_entropy.data) and not np.isinf(cross_entropy.data): losses.append(cross_entropy)\n",
    "\n",
    "    avg_loss = sum(losses)/batch_size\n",
    "    assert isinstance(avg_loss, Vector)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38fd9005-b94b-41c1-ba9e-91f67bb23958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(data=4.74130916595459, grad=0.0)\n",
      "Vector(data=4.833046913146973, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "#Will need to create a new model since log() was not part of Vector class before\n",
    "#After a bit of experimentation I've found that a two layer network is much better for some reason than a three layer network\n",
    "model2 = MLP(784, [10, 10], [Vector.tanh, Vector.softmax]) \n",
    "print(cross_entropy_loss(model2,50))\n",
    "print(cross_entropy_loss(model2,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1750c-9c88-439c-9c12-824b0cbea017",
   "metadata": {},
   "source": [
    "Right now we are having an issue because our loss function is sometimes returning a loss with data = -inf\n",
    "\n",
    "The cause of this is likely numerical instability for when we calculate the cross entropy loss. When we call pred.log() inside the function sometimes this can take the log of a negative number or 0 which can result in weirdness. A common trick to avoid this problem is to add a small epsilon to the probability vector before taking its logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85462ea1-900c-4dc9-beea-5ff52febb584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.946059703826904, accuracy: 7.000000000000001%\n",
      "step: 50, loss: 2.8334858417510986, accuracy: 17.0%\n",
      "step: 100, loss: 1.7484899759292603, accuracy: 19.0%\n",
      "step: 150, loss: 2.4760754108428955, accuracy: 23.0%\n",
      "step: 200, loss: 1.9044179916381836, accuracy: 31.0%\n",
      "step: 250, loss: 1.7274059057235718, accuracy: 32.0%\n",
      "step: 300, loss: 1.8769373893737793, accuracy: 30.0%\n",
      "step: 350, loss: 2.326725721359253, accuracy: 31.0%\n",
      "step: 400, loss: 1.5514445304870605, accuracy: 33.0%\n",
      "step: 450, loss: 1.891198754310608, accuracy: 37.0%\n",
      "step: 500, loss: 1.5463694334030151, accuracy: 40.0%\n",
      "step: 550, loss: 1.8329941034317017, accuracy: 44.0%\n",
      "step: 600, loss: 1.466937780380249, accuracy: 45.0%\n",
      "step: 650, loss: 1.4544670581817627, accuracy: 47.0%\n",
      "step: 700, loss: 1.7295666933059692, accuracy: 46.0%\n",
      "step: 750, loss: 1.4784690141677856, accuracy: 48.0%\n",
      "step: 800, loss: 1.6154508590698242, accuracy: 48.0%\n",
      "step: 850, loss: 1.1131739616394043, accuracy: 51.0%\n",
      "step: 900, loss: 1.0886684656143188, accuracy: 52.0%\n",
      "step: 950, loss: 1.2674918174743652, accuracy: 50.0%\n",
      "step: 1000, loss: 1.4295014142990112, accuracy: 51.0%\n",
      "step: 1050, loss: 1.3539822101593018, accuracy: 55.00000000000001%\n",
      "step: 1100, loss: 1.3306560516357422, accuracy: 54.0%\n",
      "step: 1150, loss: 1.0403025150299072, accuracy: 47.0%\n",
      "step: 1200, loss: 1.1123250722885132, accuracy: 53.0%\n",
      "step: 1250, loss: 1.0103269815444946, accuracy: 53.0%\n",
      "step: 1300, loss: 1.3555176258087158, accuracy: 51.0%\n",
      "step: 1350, loss: 1.020476222038269, accuracy: 56.99999999999999%\n",
      "step: 1400, loss: 1.0107369422912598, accuracy: 50.0%\n",
      "step: 1450, loss: 1.0088223218917847, accuracy: 53.0%\n",
      "step: 1500, loss: 0.9676333069801331, accuracy: 57.99999999999999%\n",
      "step: 1550, loss: 1.2519001960754395, accuracy: 55.00000000000001%\n",
      "step: 1600, loss: 0.950337290763855, accuracy: 59.0%\n",
      "step: 1650, loss: 0.8883143663406372, accuracy: 54.0%\n",
      "step: 1700, loss: 1.1038459539413452, accuracy: 63.0%\n",
      "step: 1750, loss: 1.2284119129180908, accuracy: 60.0%\n",
      "step: 1800, loss: 1.0883499383926392, accuracy: 57.99999999999999%\n",
      "step: 1850, loss: 1.1862460374832153, accuracy: 61.0%\n",
      "step: 1900, loss: 1.147963047027588, accuracy: 63.0%\n",
      "step: 1950, loss: 0.8555324673652649, accuracy: 64.0%\n",
      "step: 2000, loss: 1.1787768602371216, accuracy: 63.0%\n",
      "step: 2050, loss: 1.6622729301452637, accuracy: 62.0%\n",
      "step: 2100, loss: 1.2206833362579346, accuracy: 65.0%\n",
      "step: 2150, loss: 0.8278279900550842, accuracy: 63.0%\n",
      "step: 2200, loss: 1.4479269981384277, accuracy: 64.0%\n",
      "step: 2250, loss: 0.844973087310791, accuracy: 66.0%\n",
      "step: 2300, loss: 0.9884858131408691, accuracy: 68.0%\n",
      "step: 2350, loss: 1.154802680015564, accuracy: 66.0%\n",
      "step: 2400, loss: 1.0510715246200562, accuracy: 68.0%\n",
      "step: 2450, loss: 0.9122118949890137, accuracy: 67.0%\n",
      "step: 2500, loss: 0.7365136742591858, accuracy: 70.0%\n",
      "step: 2550, loss: 0.7771072387695312, accuracy: 69.0%\n",
      "step: 2600, loss: 0.7164081335067749, accuracy: 70.0%\n",
      "step: 2650, loss: 1.0065913200378418, accuracy: 69.0%\n",
      "step: 2700, loss: 0.9163593649864197, accuracy: 69.0%\n",
      "step: 2750, loss: 0.7996252179145813, accuracy: 69.0%\n",
      "step: 2800, loss: 1.3596607446670532, accuracy: 69.0%\n",
      "step: 2850, loss: 0.6333638429641724, accuracy: 68.0%\n",
      "step: 2900, loss: 1.1363188028335571, accuracy: 69.0%\n",
      "step: 2950, loss: 0.8618324398994446, accuracy: 71.0%\n",
      "step: 3000, loss: 1.0212461948394775, accuracy: 70.0%\n",
      "step: 3050, loss: 0.930245041847229, accuracy: 70.0%\n",
      "step: 3100, loss: 0.6371051669120789, accuracy: 70.0%\n",
      "step: 3150, loss: 0.9585602283477783, accuracy: 71.0%\n",
      "step: 3200, loss: 1.28413987159729, accuracy: 70.0%\n",
      "step: 3250, loss: 0.9758248925209045, accuracy: 71.0%\n",
      "step: 3300, loss: 0.5946608781814575, accuracy: 71.0%\n",
      "step: 3350, loss: 0.9443693161010742, accuracy: 74.0%\n",
      "step: 3400, loss: 1.047922968864441, accuracy: 70.0%\n",
      "step: 3450, loss: 0.84946209192276, accuracy: 71.0%\n",
      "step: 3500, loss: 1.2288991212844849, accuracy: 74.0%\n",
      "step: 3550, loss: 0.6209101676940918, accuracy: 76.0%\n",
      "step: 3600, loss: 0.7310988306999207, accuracy: 73.0%\n",
      "step: 3650, loss: 0.6334919333457947, accuracy: 78.0%\n",
      "step: 3700, loss: 0.8698586225509644, accuracy: 75.0%\n",
      "step: 3750, loss: 0.8605808019638062, accuracy: 72.0%\n",
      "step: 3800, loss: 0.9216281175613403, accuracy: 76.0%\n",
      "step: 3850, loss: 0.8684542179107666, accuracy: 76.0%\n",
      "step: 3900, loss: 0.5375805497169495, accuracy: 74.0%\n",
      "step: 3950, loss: 0.5868855714797974, accuracy: 74.0%\n",
      "step: 4000, loss: 0.8222426772117615, accuracy: 73.0%\n",
      "step: 4050, loss: 0.771780252456665, accuracy: 75.0%\n",
      "step: 4100, loss: 0.3990500271320343, accuracy: 74.0%\n",
      "step: 4150, loss: 0.8092479705810547, accuracy: 75.0%\n",
      "step: 4200, loss: 0.9561461210250854, accuracy: 73.0%\n",
      "step: 4250, loss: 1.0920120477676392, accuracy: 80.0%\n",
      "step: 4300, loss: 0.47849172353744507, accuracy: 77.0%\n",
      "step: 4350, loss: 0.5178472995758057, accuracy: 76.0%\n",
      "step: 4400, loss: 0.6442886590957642, accuracy: 75.0%\n",
      "step: 4450, loss: 0.578099250793457, accuracy: 77.0%\n",
      "step: 4500, loss: 0.6724749803543091, accuracy: 76.0%\n",
      "step: 4550, loss: 0.6702996492385864, accuracy: 79.0%\n",
      "step: 4600, loss: 0.5466701984405518, accuracy: 77.0%\n",
      "step: 4650, loss: 0.6708506941795349, accuracy: 77.0%\n",
      "step: 4700, loss: 0.9097540378570557, accuracy: 79.0%\n",
      "step: 4750, loss: 0.6644524335861206, accuracy: 75.0%\n",
      "step: 4800, loss: 0.7221466898918152, accuracy: 75.0%\n",
      "step: 4850, loss: 1.2571477890014648, accuracy: 79.0%\n",
      "step: 4900, loss: 0.8702110648155212, accuracy: 78.0%\n",
      "step: 4950, loss: 0.9216729998588562, accuracy: 76.0%\n"
     ]
    }
   ],
   "source": [
    "#Lets now run descent with the new model using cross entropy loss for 5000 steps \n",
    "#(thats how many new_nn has used to get to 67% right now)\n",
    "\n",
    "descent(model2, 32, 5000, loss_fn = cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d7562-b87b-473f-97c2-d2c68ed946d7",
   "metadata": {},
   "source": [
    "As we can see, using cross entropy loss is much more efficient for our model. So far the best results I have gotten is training a 2-layer network (first layer using sigmoid and second layer using softmax) for 5000 steps with a batch_size of 32. This achieved an accuracy of 83% on test data. Lets try one more time with a two layer network that trains for 20000 steps to see if we can achieve an even better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e3d0977-8480-42e1-9d8d-608f27e20958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MLP(784, [16, 10], [Vector.relu, Vector.softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fb42ca7-2f41-4fa6-a829-66c345375b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 14.317644119262695, accuracy: 14.000000000000002%\n",
      "step: 500, loss: 1.3867179155349731, accuracy: 56.00000000000001%\n",
      "step: 1000, loss: 0.9786850214004517, accuracy: 68.0%\n",
      "step: 1500, loss: 0.916029155254364, accuracy: 70.0%\n",
      "step: 2000, loss: 0.49325117468833923, accuracy: 74.0%\n",
      "step: 2500, loss: 0.7676650285720825, accuracy: 78.0%\n",
      "step: 3000, loss: 0.9640111327171326, accuracy: 81.0%\n",
      "step: 3500, loss: 0.6982080340385437, accuracy: 79.0%\n",
      "step: 4000, loss: 0.6893267035484314, accuracy: 73.0%\n",
      "step: 4500, loss: 0.26148825883865356, accuracy: 82.0%\n",
      "step: 5000, loss: 0.765138566493988, accuracy: 83.0%\n",
      "step: 5500, loss: 0.8037636876106262, accuracy: 82.0%\n",
      "step: 6000, loss: 0.39003023505210876, accuracy: 82.0%\n",
      "step: 6500, loss: 0.3556559383869171, accuracy: 82.0%\n",
      "step: 7000, loss: 0.6000338196754456, accuracy: 80.0%\n",
      "step: 7500, loss: 0.4652116000652313, accuracy: 82.0%\n",
      "step: 8000, loss: 0.5040671825408936, accuracy: 84.0%\n",
      "step: 8500, loss: 0.4210887849330902, accuracy: 82.0%\n",
      "step: 9000, loss: 0.6755635738372803, accuracy: 85.0%\n",
      "step: 9500, loss: 0.26535066962242126, accuracy: 83.0%\n",
      "step: 10000, loss: 0.19954949617385864, accuracy: 82.0%\n",
      "step: 10500, loss: 0.16453133523464203, accuracy: 86.0%\n",
      "step: 11000, loss: 0.11197032779455185, accuracy: 88.0%\n",
      "step: 11500, loss: 0.40555644035339355, accuracy: 83.0%\n",
      "step: 12000, loss: 0.40875372290611267, accuracy: 86.0%\n",
      "step: 12500, loss: 0.7150186896324158, accuracy: 85.0%\n",
      "step: 13000, loss: 0.27092722058296204, accuracy: 88.0%\n",
      "step: 13500, loss: 0.22466488182544708, accuracy: 88.0%\n",
      "step: 14000, loss: 0.964059054851532, accuracy: 88.0%\n",
      "step: 14500, loss: 0.2852323055267334, accuracy: 87.0%\n",
      "step: 15000, loss: 0.5451698899269104, accuracy: 91.0%\n",
      "step: 15500, loss: 0.5602462291717529, accuracy: 86.0%\n",
      "step: 16000, loss: 0.149746373295784, accuracy: 91.0%\n",
      "step: 16500, loss: 0.26432326436042786, accuracy: 90.0%\n",
      "step: 17000, loss: 0.42120361328125, accuracy: 92.0%\n",
      "step: 17500, loss: 0.4997579753398895, accuracy: 90.0%\n",
      "step: 18000, loss: 0.16924214363098145, accuracy: 87.0%\n",
      "step: 18500, loss: 0.39173027873039246, accuracy: 90.0%\n",
      "step: 19000, loss: 0.2196725457906723, accuracy: 91.0%\n",
      "step: 19500, loss: 0.5405247211456299, accuracy: 92.0%\n"
     ]
    }
   ],
   "source": [
    "descent(model3, 40, 20000, loss_fn = cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3aeefd2-38bd-46d4-975b-f8718650b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy:  91.0\n",
      "final training accuracy:  89.47971360381861\n"
     ]
    }
   ],
   "source": [
    "#Lets see what the final accuracy was for the testing data:\n",
    "print('final testing accuracy: ', getAccuracy(model3)*100)\n",
    "\n",
    "#And for the training data:\n",
    "print('final training accuracy: ', getAccuracy(model3, X = X_train, Y = Y_train)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6671d-dbb6-493a-9b00-c614ebc72c93",
   "metadata": {},
   "source": [
    "Not bad at all. Looks like we have finally solved the MNIST digit recognition challenge with nothing but numpy and our own custom libraries and functions!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
